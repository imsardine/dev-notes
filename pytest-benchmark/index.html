<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">  
    <link rel="canonical" href="https://imsardine.github.io/dev-notes/pytest-benchmark/">
    <link rel="shortcut icon" href="../img/favicon.ico">

    
    <title>pytest-benchmark - 建造以學習 | 在電梯裡遇見雙胞胎</title>
    

    <link rel="stylesheet" href="//use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/3.003/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="../css/base.min.css" rel="stylesheet">
    <link href="../css/cinder.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.min.css">
    <link href="../cinder_extra.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
            <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.28/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    

     
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href="..">建造以學習 | 在電梯裡遇見雙胞胎</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fas fa-search"></i> Search
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/imsardine/dev-notes/blob/source/docs/pytest-benchmark.md">Edit on imsardine/dev-notes</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#pytest-benchmark">pytest-benchmark</a></li>
            <li class="second-level"><a href="#hello-world">Hello, World!</a></li>
                
            <li class="second-level"><a href="#getting-started">新手上路</a></li>
                
            <li class="second-level"><a href="#comparsion-assertion">Comparsion &amp; Assertion ??</a></li>
                
            <li class="second-level"><a href="#setup">安裝設置</a></li>
                
            <li class="second-level"><a href="#reference">參考資料</a></li>
                
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="pytest-benchmark">pytest-benchmark<a class="headerlink" href="#pytest-benchmark" title="Permanent link"> #</a></h1>
<ul>
<li>
<p><a href="https://blog.ionelmc.ro/2015/01/12/proxying-objects-in-python/">Proxying objects in Python | ionel&rsquo;s codelog</a> (2015-01-12) 第一次在這裡看到 <code>pytest-benchmark</code>，而這份文件的作者正是 <code>pytest-benchmark</code> 的開發者。</p>
</li>
<li>
<p><a href="https://github.com/ionelmc/pytest-benchmark">ionelmc/pytest-benchmark: py.test fixture for benchmarking code</a></p>
<ul>
<li>
<p>A pytest FIXTURE for benchmarking code. It will group the tests into ROUNDS that are CALIBRATED to the chosen timer. See calibration and FAQ.</p>
<p>會有 &ldquo;rounds that are calibrated&rdquo; 的說法，是因為每個 round 要執行幾次 benchmarked function 是在 calibration phase 即使評估後決定的。</p>
</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/faq.html">Frequently Asked Questions — pytest-benchmark 3.2.2 documentation</a> #ril</p>
</li>
</ul>
<h2 id="hello-world">Hello, World!<a class="headerlink" href="#hello-world" title="Permanent link"> #</a></h2>
<pre><code>$ cat test.py
import os
import time

SLEEP_MS = int(os.getenv('SLEEP_MS')) # (1)

def benchmarked_function():
    time.sleep(SLEEP_MS / 1000.0)

def test_datetime_now(benchmark):
    benchmark(benchmarked_function)
</code></pre>

<ol>
<li>刻意用 <code>SLEEP_MS</code> 來控制執行速度，方便觀察 benchmark 的變化。</li>
</ol>
<pre><code>$ SLEEP_MS=1 pytest --benchmark-save=sleep-1ms test.py
============================================================================ test session starts ============================================================================
platform darwin -- Python 2.7.10, pytest-4.5.0, py-1.8.0, pluggy-0.11.0
benchmark: 3.2.2 (defaults: timer=time.time disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /private/tmp/perf
plugins: benchmark-3.2.2
collected 1 item

test.py .                                                                                                                                                             [100%]
Saved benchmark data in: /private/tmp/perf/.benchmarks/Darwin-CPython-2.7-64bit/0001_sleep-1ms.json (1)



-------------------------------------------- benchmark: 1 tests --------------------------------------------
Name (time in ms)        Min     Max    Mean  StdDev  Median     IQR  Outliers       OPS  Rounds  Iterations (2)
------------------------------------------------------------------------------------------------------------
test_datetime_now     1.0099  2.8200  1.2903  0.1287  1.2791  0.1118    148;61  775.0020     902           1 (3)
------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
========================================================================= 1 passed in 2.42 seconds ==========================================================================

$ SLEEP_MS=2 pytest --benchmark-save=sleep-2ms test.py
============================================================================ test session starts ============================================================================
platform darwin -- Python 2.7.10, pytest-4.5.0, py-1.8.0, pluggy-0.11.0
benchmark: 3.2.2 (defaults: timer=time.time disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /private/tmp/perf
plugins: benchmark-3.2.2
collected 1 item

test.py .                                                                                                                                                             [100%]
Saved benchmark data in: /private/tmp/perf/.benchmarks/Darwin-CPython-2.7-64bit/0002_sleep-2ms.json



-------------------------------------------- benchmark: 1 tests --------------------------------------------
Name (time in ms)        Min     Max    Mean  StdDev  Median     IQR  Outliers       OPS  Rounds  Iterations
------------------------------------------------------------------------------------------------------------
test_datetime_now     2.0211  3.2320  2.5328  0.1702  2.6190  0.1232     60;57  394.8176     381           1 (4)
------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
========================================================================= 1 passed in 2.21 seconds ==========================================================================
</code></pre>

<ol>
<li>因為 <code>--benchmark-save=sleep-1ms</code> 的關係，benchmark 的結果會寫到 <code>./benchmarks/.../NNNN_sleep-1ms.json</code>，之後可以與其他 benchmark 比較。</li>
<li>Name (time in ms) 提示了 result table 中各項數據的單位 &ndash; 會隨著 benchmarked 的快慢而異，可能是 ns (nanosecond)、us (microsecond)、ms (millisecond) 或 s (second) 等。</li>
<li>中間暫停 1ms &ndash; 每個 round 執行 1 次 benchmarked iteration (因為 1ms 已經大於 10 x timer resolution)，總共執行了 902 次。</li>
<li>中間暫停 2ms &ndash; 每個 round 執行 1 次 benchmarked iteration，總共執行了 381 次；初步看來，時間差不多就是第一次的 2 倍。</li>
</ol>
<pre><code>$ pytest-benchmark compare

--------------------------------------------------------------------------------------- benchmark: 2 tests --------------------------------------------------------------------------------------
Name (time in ms)                       Min               Max              Mean            StdDev            Median               IQR            Outliers       OPS            Rounds  Iterations
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_datetime_now (0001_sleep-1)     1.0099 (1.0)      2.8200 (1.0)      1.2903 (1.0)      0.1287 (1.0)      1.2791 (1.0)      0.1118 (1.0)        148;61  775.0020 (1.0)         902           1 (1)
test_datetime_now (0002_sleep-2)     2.0211 (2.00)     3.2320 (1.15)     2.5328 (1.96)     0.1702 (1.32)     2.6190 (2.05)     0.1232 (1.10)        60;57  394.8176 (0.51)        381           1 (2)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
</code></pre>

<ol>
<li>中間暫停 1ms；第一行數據是基準，所以各項數據後面都是 (1.0)，表示倍率。</li>
<li>中間佔停 2ms；各項數據後面是基準的倍數，Min、Max、Mean、Median 符合預期都在 2.0 左右。</li>
</ol>
<h2 id="getting-started">新手上路<a class="headerlink" href="#getting-started" title="Permanent link"> #</a></h2>
<ul>
<li>
<p><a href="https://subscription.packtpub.com/book/application_development/9781787282896/1/ch01lvl1sec2/better-tests-and-benchmarks-with-pytest-benchmark">Better tests and benchmarks with pytest-benchmark - Python High Performance - Second Edition</a></p>
<ul>
<li>The Unix <code>time</code> command is a versatile tool that can be used to assess the running time of small programs on a variety of platforms. For larger Python applications and libraries, a more comprehensive solution that deals with BOTH TESTING AND BENCHMARKING is <code>pytest</code>, in combination with its <code>pytest-benchmark</code> plugin.</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/calibration.html">Calibration — pytest-benchmark 3.2.2 documentation</a></p>
<p>先瞭解 calibration 跟 timer resolution 的問題，接下來的 Glossary 才知道在講什麼，也才看得懂 result table 中各項數據的意義。</p>
<ul>
<li>
<p><code>pytest-benchmark</code> will run your function MULTIPLE TIMES BETWEEN MEASUREMENTS. A round is that SET OF RUNS done between measurements. This is quite similar to the builtin <code>timeit</code> module but it’s MORE ROBUST.</p>
<p>其中的 run 在 pytest-benchmark 裡有另一個 iteration 的說法，指的都是 benchmarked function 執行一次。</p>
</li>
<li>
<p>The problem with measuring SINGLE RUNS appears when you have VERY FAST CODE. To illustrate:</p>
<p><img alt="" src="https://github.com/ionelmc/pytest-benchmark/raw/master/docs/measurement-issues.png" /></p>
<p>關鍵在左上方的 Timer resolution: 500ns，所以最上面 Benchmark A &amp; B 不足 500ns 被視為 0ns，中間 Benchmark C &amp; D 不足 1µs 視為 500ns。</p>
<p>注意下方 Benchmark A &amp; C 開始的時間比較晚，因為 Benchmark A 沒跨過 1µs 但 Benchmark C 有跨過，所以就有 500ns 與 1µs 的差別。</p>
</li>
<li>
<p>In other words, a round is a set of runs that are AVERAGED TOGETHER, those resulting numbers are then used to compute the result tables.</p>
<p>The default settings will try to KEEP THE ROUND SMALL ENOUGH (so that you get to see variance), but not too small, because then you have the TIMER CALIBRATION ISSUES illustrated above (your test function is FASTER THAN OR AS FAST AS THE RESOLUTION OF THE TIMER).</p>
<p>由於 benchmarked function 單次執行時間可能遠低於 timer resolution，所以每個 round 會連續執行多次使總執行時間貼近 10 倍 timer resolution，再透過平均值求取單次的執行時間；多個 round 就有多個數據，也就可以算 min、max、mean、stddev、median 等。</p>
<p>而 calibration phase 就是試著執行看看 benchmarked function 以決定一個 round 要執行幾次，才能遶開 timer resolution 可能不夠細的問題。</p>
</li>
<li>
<p>By default <code>pytest-benchmark</code> will try to run your function AS MANY TIMES NEEDED TO FIT A 10 x <code>TIMER_RESOLUTION</code> period. You can fine tune this with the <code>--benchmark-min-time</code> and <code>--benchmark-calibration-precision</code> options.</p>
</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/glossary.html">Glossary — pytest-benchmark 3.2.2 documentation</a></p>
<p>要先有 calibration 的基礎。</p>
<ul>
<li>
<p>Iteration</p>
<p>A SINGLE RUN of your benchmarked function.</p>
</li>
<li>
<p>Round</p>
<p>A SET OF ITERATIONS. The size of a round is COMPUTED IN THE CALIBRATION PHASE.</p>
<p>Stats are computed with rounds, not with iterations. The duration for a round is an AVERAGE of all the iterations in that round.</p>
<p>See: Calibration for an explanation of why it’s like this.</p>
</li>
<li>
<p>Mean</p>
<p>TODO: 平均值</p>
</li>
<li>
<p>Median</p>
<p>TODO: 中位數</p>
</li>
<li>
<p>IQR</p>
<p>InterQuertile Range. This is a different way to measure VARIANCE. Good explanation <a href="https://www.dataz.io/display/Public/2013/03/20/Describing+Data%3A+Why+median+and+IQR+are+often+better+than+mean+and+standard+deviation">here</a> #ril
      - StdDev</p>
<p>TODO: Standard Deviation</p>
</li>
<li>
<p>Outliers (TODO)</p>
</li>
</ul>
</li>
<li>
<p><a href="https://github.com/ionelmc/pytest-benchmark#examples">Examples - ionelmc/pytest-benchmark: py.test fixture for benchmarking code</a></p>
<p>But first, a prologue:</p>
<ul>
<li>
<p>This plugin TIGHTLY INTEGRATES into pytest. To use this effectively you should know a thing or two about pytest first. Take a look at the introductory material or watch talks.</p>
<p>事實上 <code>pytest-benchmark</code> 對 <code>pytest</code> 的版本也很要求，否則會遇到累似下面的錯誤：</p>
<pre><code>pluggy.manager.PluginValidationError: Plugin 'benchmark' could not be loaded: (pytest 3.7.1 (/tmp/tox/py3/lib/python3.7/site-packages), Requirement.parse('pytest&gt;=3.8'))!
</code></pre>
<p>Few notes:</p>
<ul>
<li>This plugin benchmarks FUNCTIONS and only that. If you want to measure block of code or whole programs you will need to write a WRAPPER FUNCTION.</li>
<li>In a test you can only benchmark ONE FUNCTION. If you want to benchmark many functions write more tests or use <a href="http://docs.pytest.org/en/latest/parametrize.html">parametrization</a>.</li>
<li>To run the benchmarks you simply use pytest to run your &ldquo;tests&rdquo;. The plugin will automatically do the benchmarking and generate a RESULT TABLE. Run <code>pytest --help</code> for more details.</li>
</ul>
<p>This plugin provides a <code>benchmark</code> FIXTURE. This fixture is a CALLABLE object that will benchmark any function passed to it.</p>
<p>因為是 fixture，所以不用 import。</p>
</li>
<li>
<p>Example:</p>
<pre><code>def something(duration=0.000001):
    """
    Function that needs some serious benchmarking.
    """
    time.sleep(duration)
    # You may return anything you want, like the result of a computation
    return 123

def test_my_stuff(benchmark):
    # benchmark something
    result = benchmark(something) # 這樣是執行幾次 ??

    # Extra code, to verify that the run completed correctly.
    # Sometimes you may want to check the result, fast functions
    # are no good if they return incorrect results :-)
    assert result == 123 # 為何不是 assert 執行時間低於某個標準 ??
</code></pre>
</li>
<li>
<p>You can also pass extra arguments:</p>
<pre><code>def test_my_stuff(benchmark):
    benchmark(time.sleep, 0.02)
</code></pre>
<p>Or even keyword arguments:</p>
<pre><code>def test_my_stuff(benchmark):
    benchmark(time.sleep, duration=0.02)
</code></pre>
</li>
<li>
<p>Another pattern seen in the wild, that is not recommended for MICRO-BENCHMARKS (very fast code) but may be convenient:</p>
<pre><code>def test_my_stuff(benchmark):
    @benchmark
    def something():  # unnecessary function call
        time.sleep(0.000001)
</code></pre>
<p>A better way is to just benchmark THE FINAL FUNCTION:</p>
<pre><code>def test_my_stuff(benchmark):
    benchmark(time.sleep, 0.000001)  # way more accurate results!
</code></pre>
</li>
<li>
<p>If you need to do fine control over how the benchmark is run (like a setup function, exact control of iterations and rounds) there&rsquo;s a special mode - pedantic:</p>
<pre><code>def my_special_setup():
    ...

def test_with_setup(benchmark):
    benchmark.pedantic(something, setup=my_special_setup, args=(1, 2, 3), kwargs={'foo': 'bar'}, iterations=10, rounds=100)
</code></pre>
</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/usage.html">Usage — pytest-benchmark 3.2.2 documentation</a> #ril</p>
</li>
<li><a href="https://subscription.packtpub.com/book/application_development/9781787282896/1/ch01lvl1sec1/writing-tests-and-benchmarks">Writing tests and benchmarks - Python High Performance - Second Edition</a> #ril</li>
<li><a href="https://pytest-benchmark.readthedocs.io/en/latest/pedantic.html">Pedantic mode — pytest-benchmark 3.2.2 documentation</a> #ril</li>
</ul>
<h2 id="comparsion-assertion">Comparsion &amp; Assertion ??<a class="headerlink" href="#comparsion-assertion" title="Permanent link"> #</a></h2>
<ul>
<li>
<p>在硬體條件不變的情況下，比較不同次 benchmark 的結果才有意義。</p>
<p>以 GitLab CI 為例，每個 build 的測試都可能跑在不同的 runner 上，比較不同 build 的 benchmark 就不太有意義？</p>
</li>
</ul>
<hr />
<p>參考資料：</p>
<ul>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/comparing.html">Comparing past runs — pytest-benchmark 3.2.2 documentation</a> #ril</p>
<ul>
<li>
<p>Before comparing different runs it’s ideal to make your tests as CONSISTENT as possible, see Frequently Asked Questions for more details.</p>
</li>
<li>
<p><code>pytest-benchmark</code> has support for storing stats and data for the previous runs.</p>
<p>To store a run just add <code>--benchmark-autosave</code> or <code>--benchmark-save=some-name</code> to your <code>pytest</code> arguments. All the files are saved in a path like <code>.benchmarks/Linux-CPython-3.4-64bit</code>.</p>
<ul>
<li>
<p><code>--benchmark-autosave</code> saves a file like <code>0001_c9cca5de6a4c7eb2_20150815_215724.json</code> where:</p>
<ul>
<li>
<p>0001 is an automatically incremented id, much like how django migrations have a number.</p>
</li>
<li>
<p>c9cca5de6a4c7eb2 is the commit id (if you use Git or Mercurial)</p>
<p>在本地端來回試幾個 commit 跟 benchmark 的關係時很方便。</p>
</li>
<li>
<p>20150815_215724 is the current time</p>
</li>
</ul>
<p>You should add <code>--benchmark-autosave</code> to <code>addopts</code> in you pytest configuration so you dont have to specify it all the time.</p>
</li>
<li>
<p><code>--benchmark-name=foobar</code> works similarly, but saves a file like <code>0001_foobar.json</code>. It’s there in case you want to give specific name to the run.</p>
</li>
</ul>
</li>
<li>
<p>After you have saved your first run you can compare against it with <code>--benchmark-compare=0001</code>. You will get an additional row for each test in the result table, showing the differences.</p>
</li>
<li>
<p>You can also make the suite fail with <code>--benchmark-compare-fail=&lt;stat&gt;:&lt;num&gt;%</code> or <code>--benchmark-compare-fail=&lt;stat&gt;:&lt;num&gt;</code>. Examples:</p>
<ul>
<li><code>--benchmark-compare-fail=min:5%</code> will make the suite fail if Min is 5% slower for any test.</li>
<li><code>--benchmark-compare-fail=mean:0.001</code> will make the suite fail if Mean is 0.001 seconds slower for any test.</li>
</ul>
<p>可以在 repository 裡固定放個 <code>./benchmarks/.../0001_xxx</code> 做為 baseline，不過機器可能不同問題要如何解?? &rarr; 透過 label 指定執行機器</p>
</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/usage.html#commandline-options">Commandline options - Usage — pytest-benchmark 3.2.2 documentation</a></p>
<ul>
<li>
<p><code>--benchmark-compare=NUM</code></p>
<p>Compare the current run against run <code>NUM</code> (or prefix of <code>_id</code> in elasticsearch) or the LATEST SAVED RUN if unspecified.</p>
<p>原來 <code>--benchmark-storage=URI</code> 可以將測試結果存到 Elasticsearch #ril</p>
</li>
<li>
<p><code>--benchmark-compare-fail=EXPR</code></p>
<p>Fail test if performance REGRESSES according to given <code>EXPR</code> (eg: <code>min:5%</code> or <code>mean:0.001</code> for number of SECONDS). Can be used multiple times.</p>
</li>
</ul>
</li>
<li>
<p><a href="https://pytest-benchmark.readthedocs.io/en/latest/usage.html#comparison-cli">Comparison CLI - Usage — pytest-benchmark 3.2.2 documentation</a> #ril</p>
</li>
<li><a href="https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/fixture.py#L65">pytest-benchmark/fixture.py at master · ionelmc/pytest-benchmark</a> <code>benchmark.stats</code> 似乎可以用來判定結果 #ril</li>
<li><a href="https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/stats.py">https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/stats.py</a> #ril</li>
<li><a href="https://github.com/ionelmc/pytest-benchmark/blob/master/src/pytest_benchmark/stats.py">pytest-benchmark/stats.py at master · ionelmc/pytest-benchmark</a> 記錄了 benchmark 的結果 #ril</li>
</ul>
<h2 id="setup">安裝設置<a class="headerlink" href="#setup" title="Permanent link"> #</a></h2>
<ul>
<li><a href="https://github.com/ionelmc/pytest-benchmark#installation">Installation - ionelmc/pytest-benchmark: py.test fixture for benchmarking code</a><pre><code>pip install pytest-benchmark
</code></pre>
</li>
</ul>
<h2 id="reference">參考資料<a class="headerlink" href="#reference" title="Permanent link"> #</a></h2>
<ul>
<li><a href="https://github.com/ionelmc/pytest-benchmark">ionelmc/pytest-benchmark</a></li>
<li><a href="https://pypi.org/project/pytest-benchmark/">pytest-benchmark - PyPI</a></li>
</ul>
<p>文件：</p>
<ul>
<li><a href="https://pytest-benchmark.readthedocs.io/en/latest/">pytest-benchmark Documentation</a></li>
</ul>
<p>手冊：</p>
<ul>
<li><a href="https://pytest-benchmark.readthedocs.io/en/latest/usage.html#commandline-options">Commandline Options</a></li>
</ul></div>
        
        
    </div>

    <footer class="col-md-12 text-center">
        
        <hr>
        <p>
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
        
        
    </footer>

    <script src="../js/jquery-3.3.1.min.js"></script>
    <script src="../js/bootstrap-3.0.3.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '..';
    </script>
    <!-- <script data-main="../mkdocs/js/search.js" src="../mkdocs/js/require.js"></script> -->
    <script src="../js/base.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
    <script src="../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
